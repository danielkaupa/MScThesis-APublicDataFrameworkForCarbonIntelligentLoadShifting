{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281cdf52",
   "metadata": {},
   "source": [
    "# Optimising Demand Response Strategies for Carbon Intelligent Load Shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a9580",
   "metadata": {},
   "source": [
    "# Investigating ERA5 World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59999f02",
   "metadata": {},
   "source": [
    "**NOTEBOOK PURPOSE(S):**\n",
    "* Investigate structure and contents of data retrieved from the ERA5 World dataset.\n",
    "\n",
    "**NOTEBOOK OUTPUTS:**\n",
    "* N/A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcc436",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3486c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Data Manipulation & Analysis\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Geospatial Data Handling\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import xarray as xr\n",
    "import pygrib\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Notebook/Display Tools\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from IPython.display import display\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# System / Miscellaneous\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724ce59",
   "metadata": {},
   "source": [
    "### Loading Data from Local Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea5cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Current Working Directory and contents:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/IRP_WORK_UPDATED.nosync/new_repo/MSc-Thesis-OptimisingDemandResponseStrategiesForCarbonIntelligentElectricityUse/code_and_analysis/jupyter_notebooks\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: .DS_Store\n",
      "-> File: marginal_emissions_data_prep.ipynb\n",
      "-> File: marginal_emissions_model_development.ipynb\n",
      "-> File: step1_hitachi_data_retrieval.ipynb\n",
      "-> File: step2_initial_data_analysis.ipynb\n",
      "-> File: step2_investigating_data_size_and_types.ipynb\n",
      "-> File: step2_investigating_era5_world_data.ipynb\n",
      "-> File: step3_combining_era5_datasets.ipynb\n",
      "-> File: step3_combining_grid_and_weather_data.ipynb\n",
      "-> File: step3_processing_era5_land_data.ipynb\n",
      "-> File: step3_processing_era5_world_data.ipynb\n",
      "-> File: step3_processing_grid_readings_data.ipynb\n",
      "-> File: step4_additional_model_development.ipynb\n",
      "-> File: step4_marginal_emissions_binning_models.ipynb\n",
      "-> File: step4_marginal_emissions_eda.ipynb\n",
      "-> File: step4_marginal_emissions_model_exploration.ipynb\n",
      "-> File: step5_dev_models_revamp.ipynb\n",
      "-> File: step5_developing_optimisation_models.ipynb\n",
      "-> File: step5_joining_customer_meter_emissions_test.ipynb\n",
      "-> File: step6_emission_factors_analysis.ipynb\n",
      "-> File: step6_optimisation_results_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"-\"*120)\n",
    "print(\"Current Working Directory and contents:\\n\"+\"-\"*120)\n",
    "for root, dirs, files in os.walk(cwd):\n",
    "    print(f\"\\nDirectory: {root}\")\n",
    "    print(f\"Subdirectories: {dirs}\\n\"+ \"-\"*40)\n",
    "    for file in sorted(files):\n",
    "        print(f\"-> File: {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d10a0",
   "metadata": {},
   "source": [
    "### Defining File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of data directory structure and contents\n",
    "root_directory = os.path.join('..', '..')\n",
    "\n",
    "# Base data directory\n",
    "base_data_directory = os.path.join(root_directory, \"data\")\n",
    "\n",
    "# Directory where the dataframes will be saved\n",
    "hitachi_data_directory = os.path.join(base_data_directory, 'hitachi')\n",
    "\n",
    "# ERA5 data directory\n",
    "era5_data_directory = os.path.join(base_data_directory, \"era5\")  # Directory where the ERA5 data is stored\n",
    "grib_era5_data_directory = os.path.join(era5_data_directory, \"grib_downloads\")  # Directory where the ERA5 grib files will be saved\n",
    "parquets_era5_data_directory = os.path.join(era5_data_directory, \"parquets\")  # Directory where the ERA5 parquet files will be saved\n",
    "\n",
    "# Directory where the outputs are saved\n",
    "outputs_directory = os.path.join(root_directory, 'outputs')\n",
    "outputs_metrics_directory = os.path.join(outputs_directory, 'metrics')\n",
    "outputs_images_directory = os.path.join(outputs_directory, 'images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576635bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[grib_era5_data_directory] and contents:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Directory: ../../data/era5/grib_downloads\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: 125ae282169904325e8bc153160be150.grib\n",
      "-> File: 289f2aac241f8a158ff074a66682452e.grib\n",
      "-> File: 554832a6209258041784298e5401a7ab.grib\n",
      "-> File: 5aee58993569287064988fbc8ad385dd.grib\n",
      "-> File: 5bcc58c42bdde8ce6b147b00099404bc.grib\n",
      "-> File: ad36c26a5d6daae43c9aeab1747e078c.grib\n",
      "-> File: b4eac1bff8a020500806be638e9d4ab9.grib\n",
      "-> File: bc20f736fa82ab5167820d9116ab4859.grib\n",
      "-> File: c8a985ffc4908e6597c4498ff596cbad.grib\n",
      "-> File: d1313a3f750d6e7bd89dff34b112d8a8.grib\n",
      "-> File: de87f0d77e8aeed868c68ac0daae3dc9.grib\n",
      "-> File: e23fa435dfdf294eba51378e96410b31.grib\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*120)\n",
    "print(\"[grib_era5_data_directory] and contents:\\n\"+\"-\"*120)\n",
    "for root, dirs, files in os.walk(grib_era5_data_directory):\n",
    "    print(f\"\\nDirectory: {root}\")\n",
    "    print(f\"Subdirectories: {dirs}\\n\"+ \"-\"*40)\n",
    "    for file in sorted(files):\n",
    "        print(f\"-> File: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911e0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"125ae282169904325e8bc153160be150.grib\"\n",
    "file2 = \"289f2aac241f8a158ff074a66682452e.grib\"\n",
    "file3 = \"554832a6209258041784298e5401a7ab.grib\"\n",
    "file4 = \"5aee58993569287064988fbc8ad385dd.grib\"\n",
    "file5 = \"5bcc58c42bdde8ce6b147b00099404bc.grib\"\n",
    "file6 = \"ad36c26a5d6daae43c9aeab1747e078c.grib\"\n",
    "file7 = \"b4eac1bff8a020500806be638e9d4ab9.grib\"\n",
    "file8 = \"bc20f736fa82ab5167820d9116ab4859.grib\"\n",
    "file9 = \"c8a985ffc4908e6597c4498ff596cbad.grib\"\n",
    "file10 = \"d1313a3f750d6e7bd89dff34b112d8a8.grib\"\n",
    "file11 = \"de87f0d77e8aeed868c68ac0daae3dc9.grib\"\n",
    "file12 = \"e23fa435dfdf294eba51378e96410b31.grib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013b1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of the files\n",
    "files = [\n",
    "    file1, file2, file3, file4, file5,\n",
    "    file6, file7, file8, file9, file10,\n",
    "    file11, file12\n",
    "]\n",
    "\n",
    "# create full paths for each of the files\n",
    "grib_files = [os.path.join(grib_era5_data_directory, file) for file in files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a7f95",
   "metadata": {},
   "source": [
    "### Inspecting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a73814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_with_pygrib(path):\n",
    "    with pygrib.open(path) as grbs:\n",
    "        msgs = list(grbs)  # load all messages into memory\n",
    "    # 1) collect unique variable shortNames\n",
    "    vars_ = sorted({m.shortName for m in msgs})\n",
    "    # 2) collect all validDates and then get min/max\n",
    "    dates = sorted({m.validDate for m in msgs})\n",
    "    t0, t1 = dates[0], dates[-1]\n",
    "    # 3) get lat/lon bounds from the first message\n",
    "    lat, lon = msgs[0].latlons()\n",
    "    lat0, lat1 = float(lat.min()), float(lat.max())\n",
    "    lon0, lon1 = float(lon.min()), float(lon.max())\n",
    "    # print summary\n",
    "    print(f\"\\nFile: \\'{path}\\'\")\n",
    "    print(f\"\\t- Variables: [{', '.join(vars_)}]\")\n",
    "    print(f\"\\t- Time range: [{t0.isoformat()}]  to  [{t1.isoformat()}]\")\n",
    "    print(f\"\\t- Latitude:  [{lat0:.3f}]  to  [{lat1:.3f}]\")\n",
    "    print(f\"\\t- Longitude: [{lon0:.3f}]  to  [{lon1:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044170bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: '../../data/era5/grib_downloads/125ae282169904325e8bc153160be150.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2024-12-31T18:00:00]  to  [2025-07-11T05:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/289f2aac241f8a158ff074a66682452e.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2020-12-31T18:00:00]  to  [2021-12-31T23:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/554832a6209258041784298e5401a7ab.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2023-12-31T18:00:00]  to  [2024-12-31T23:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/5aee58993569287064988fbc8ad385dd.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2023-12-31T18:00:00]  to  [2024-12-31T23:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/5bcc58c42bdde8ce6b147b00099404bc.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2022-12-31T18:00:00]  to  [2023-12-31T23:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/ad36c26a5d6daae43c9aeab1747e078c.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2021-12-31T18:00:00]  to  [2022-12-31T23:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/b4eac1bff8a020500806be638e9d4ab9.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2019-12-31T18:00:00]  to  [2020-12-31T23:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/bc20f736fa82ab5167820d9116ab4859.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2019-12-31T18:00:00]  to  [2020-12-31T23:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/c8a985ffc4908e6597c4498ff596cbad.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2022-12-31T18:00:00]  to  [2023-12-31T23:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/d1313a3f750d6e7bd89dff34b112d8a8.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2020-12-31T18:00:00]  to  [2021-12-31T23:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/de87f0d77e8aeed868c68ac0daae3dc9.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2024-12-31T18:00:00]  to  [2025-07-10T19:00:00]\n",
      "\t- Latitude:  [17.000]  to  [21.000]\n",
      "\t- Longitude: [70.000]  to  [74.000]\n",
      "\n",
      "File: '../../data/era5/grib_downloads/e23fa435dfdf294eba51378e96410b31.grib'\n",
      "\t- Variables: [10u, 10v, 2t, hcc, lcc, mcc, ssr, ssrd, tcc, tp]\n",
      "\t- Time range: [2021-12-31T18:00:00]  to  [2022-12-31T23:00:00]\n",
      "\t- Latitude:  [26.000]  to  [30.000]\n",
      "\t- Longitude: [75.000]  to  [79.000]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for f in grib_files:\n",
    "    inspect_with_pygrib(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6feedee",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "The city of Delhi is covered by\n",
    " - Latitude:  [26.000]  to  [30.000]\n",
    " - Longitude: [75.000]  to  [79.000]\n",
    "\n",
    "The city of Mumbai is covered by\n",
    " - Latitude:  [17.000]  to  [21.000]\n",
    " - Longitude: [70.000]  to  [74.000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bb6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the values from the above into corresponding file names\n",
    "file_2025_delhi_raw_filename = \"125ae282169904325e8bc153160be150.grib\"\n",
    "file_2021_mumbai_raw_filename = \"289f2aac241f8a158ff074a66682452e.grib\"\n",
    "file_2024_delhi_raw_filename = \"554832a6209258041784298e5401a7ab.grib\"\n",
    "file_2024_mumbai_raw_filename = \"5aee58993569287064988fbc8ad385dd.grib\"\n",
    "file_2023_delhi_raw_filename = \"5bcc58c42bdde8ce6b147b00099404bc.grib\"\n",
    "file_2022_mumbai_raw_filename = \"ad36c26a5d6daae43c9aeab1747e078c.grib\"\n",
    "file_2020_delhi_raw_filename = \"b4eac1bff8a020500806be638e9d4ab9.grib\"\n",
    "file_2020_mumbai_raw_filename = \"bc20f736fa82ab5167820d9116ab4859.grib\"\n",
    "file_2023_mumbai_raw_filename = \"c8a985ffc4908e6597c4498ff596cbad.grib\"\n",
    "file_2021_delhi_raw_filename = \"d1313a3f750d6e7bd89dff34b112d8a8.grib\"\n",
    "file_2025_mumbai_raw_filename = \"de87f0d77e8aeed868c68ac0daae3dc9.grib\"\n",
    "file_2022_delhi_raw_filename = \"e23fa435dfdf294eba51378e96410b31.grib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95af6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of files\n",
    "named_files = [file_2025_delhi_raw_filename, file_2024_delhi_raw_filename, file_2023_delhi_raw_filename, file_2022_delhi_raw_filename,\n",
    "               file_2021_delhi_raw_filename, file_2020_delhi_raw_filename, file_2025_mumbai_raw_filename, file_2024_mumbai_raw_filename,\n",
    "               file_2023_mumbai_raw_filename, file_2022_mumbai_raw_filename, file_2021_mumbai_raw_filename, file_2020_mumbai_raw_filename]\n",
    "\n",
    "# creating a list containing full paths for each of the named files\n",
    "named_filepaths = [os.path.join(grib_era5_data_directory, file) for file in named_files]\n",
    "\n",
    "# creating a list of names that will be used as keys for a dictionary of the files\n",
    "dictionary_keys = [\n",
    "    \"2025_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2024_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2023_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2022_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2021_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2020_delhi_era5_reanalysis_single_levels_data\",\n",
    "    \"2025_mumbai_era5_reanalysis_single_levels_data\",\n",
    "    \"2024_mumbai_era5_reanalysis_single_levels_data\",\n",
    "    \"2023_mumbai_era5_reanalysis_single_levels_data\",\n",
    "    \"2022_mumbai_era5_reanalysis_single_levels_data\",\n",
    "    \"2021_mumbai_era5_reanalysis_single_levels_data\",\n",
    "    \"2020_mumbai_era5_reanalysis_single_levels_data\",\n",
    "]\n",
    "\n",
    "# creating a dictionary that maps the keys to the full file paths\n",
    "named_filepaths_dictionary = dict(zip(dictionary_keys, named_filepaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49f54d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Testing named_filepaths_dictionary:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "2025 Delhi ERA5 Reanalysis Single Levels Data File Path:\n",
      "\texpected: [data/era5/grib_downloads/125ae282169904325e8bc153160be150.grib]\n",
      "\tactual: [../../data/era5/grib_downloads/125ae282169904325e8bc153160be150.grib]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2023 Mumbai ERA5 Reanalysis Single Levels Data File Path:\n",
      "\texpected: [data/era5/grib_downloads/c8a985ffc4908e6597c4498ff596cbad.grib]\n",
      "\tactual: [../../data/era5/grib_downloads/c8a985ffc4908e6597c4498ff596cbad.grib]\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*120)\n",
    "print(\"Testing named_filepaths_dictionary:\\n\"+\"-\"*120)\n",
    "\n",
    "print(\"2025 Delhi ERA5 Reanalysis Single Levels Data File Path:\")\n",
    "print(\"\\texpected: [data/era5/grib_downloads/125ae282169904325e8bc153160be150.grib]\")\n",
    "print(f\"\\tactual: [{named_filepaths_dictionary['2025_delhi_era5_reanalysis_single_levels_data']}]\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "print(\"2023 Mumbai ERA5 Reanalysis Single Levels Data File Path:\")\n",
    "print(\"\\texpected: [data/era5/grib_downloads/c8a985ffc4908e6597c4498ff596cbad.grib]\")\n",
    "print(f\"\\tactual: [{named_filepaths_dictionary['2023_mumbai_era5_reanalysis_single_levels_data']}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484bd32",
   "metadata": {},
   "source": [
    "### Investigating Resolution and Granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac90aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list = ['10u','10v','2t','hcc','lcc','mcc','ssr','ssrd','tcc','tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb064bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/cfgrib/xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n"
     ]
    }
   ],
   "source": [
    "# Reminder of the names of the structures:\n",
    "# list: named_filepaths\n",
    "# dictionary: named_filepaths_dictionary\n",
    "\n",
    "\n",
    "# we'll just use the first file as an example\n",
    "fpath = named_filepaths[0]\n",
    "\n",
    "# temporary storage for results of this investigation\n",
    "records = []\n",
    "# loop through each variable in the list\n",
    "for var in vars_list:\n",
    "    try:\n",
    "        ds = xr.open_dataset(\n",
    "            fpath,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"filter_by_keys\": {\"shortName\": var}}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {var!r}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # retrieving spatial coordinates\n",
    "    lat = ds.latitude.values\n",
    "    lon = ds.longitude.values\n",
    "\n",
    "    # flatten valid_time if there's a step dimension\n",
    "    if \"step\" in ds.dims and \"valid_time\" in ds.coords:\n",
    "        times = np.unique(ds.valid_time.values)\n",
    "    else:\n",
    "        times = ds.time.values\n",
    "\n",
    "    # compute resolutions\n",
    "    lat_res = np.unique(np.diff(lat)).round(6).tolist()\n",
    "    lon_res = np.unique(np.diff(lon)).round(6).tolist()\n",
    "\n",
    "    # temporal resolution in hours\n",
    "    dt_hrs = (np.diff(times).astype(\"timedelta64[h]\") / np.timedelta64(1, \"h\")).astype(int)\n",
    "    time_res = np.unique(dt_hrs).tolist()\n",
    "\n",
    "    # appending the results to records\n",
    "    records.append({\n",
    "        \"variable\":   var,\n",
    "        \"n_lat\":      lat.size,\n",
    "        \"lat_res°\":   lat_res,\n",
    "        \"n_lon\":      lon.size,\n",
    "        \"lon_res°\":   lon_res,\n",
    "        \"n_time\":     times.size,\n",
    "        \"time_res_h\": time_res,\n",
    "        \"t0\":         str(times.min()),\n",
    "        \"t1\":         str(times.max()),\n",
    "    })\n",
    "    # close the dataset to free resources\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a5af1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>n_lat</th>\n",
       "      <th>lat_res°</th>\n",
       "      <th>n_lon</th>\n",
       "      <th>lon_res°</th>\n",
       "      <th>n_time</th>\n",
       "      <th>time_res_h</th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10u</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10v</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2t</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hcc</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lcc</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mcc</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ssr</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4596</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2024-12-31T19:00:00.000000000</td>\n",
       "      <td>2025-07-11T06:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ssrd</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4596</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2024-12-31T19:00:00.000000000</td>\n",
       "      <td>2025-07-11T06:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tcc</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4590</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-01-01T00:00:00.000000000</td>\n",
       "      <td>2025-07-11T05:00:00.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tp</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>4596</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2024-12-31T19:00:00.000000000</td>\n",
       "      <td>2025-07-11T06:00:00.000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable  n_lat lat_res°  n_lon lon_res°  n_time time_res_h  \\\n",
       "0      10u     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "1      10v     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "2       2t     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "3      hcc     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "4      lcc     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "5      mcc     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "6      ssr     17  [-0.25]     17   [0.25]    4596        [1]   \n",
       "7     ssrd     17  [-0.25]     17   [0.25]    4596        [1]   \n",
       "8      tcc     17  [-0.25]     17   [0.25]    4590        [1]   \n",
       "9       tp     17  [-0.25]     17   [0.25]    4596        [1]   \n",
       "\n",
       "                              t0                             t1  \n",
       "0  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "1  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "2  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "3  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "4  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "5  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "6  2024-12-31T19:00:00.000000000  2025-07-11T06:00:00.000000000  \n",
       "7  2024-12-31T19:00:00.000000000  2025-07-11T06:00:00.000000000  \n",
       "8  2025-01-01T00:00:00.000000000  2025-07-11T05:00:00.000000000  \n",
       "9  2024-12-31T19:00:00.000000000  2025-07-11T06:00:00.000000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build and display the summary table\n",
    "df = pd.DataFrame(records, columns=[\n",
    "    \"variable\",\"n_lat\",\"lat_res°\",\"n_lon\",\"lon_res°\",\n",
    "    \"n_time\",\"time_res_h\",\"t0\",\"t1\"\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee898ed",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1e2b6",
   "metadata": {},
   "source": [
    "**Data Structure and Resolution**\n",
    "- The ERA5-World reanalysis data covers Delhi and Mumbai regions with a spatial resolution of ~0.25 degrees (~30km)\n",
    "- This resolution is adequate for regional weather patterns but may miss urban microclimate effects\n",
    "- The data contains hourly measurements from 2020-2025, offering high temporal granularity\n",
    "\n",
    "**Weather Variables**\n",
    "- The dataset includes key meteorological variables: \n",
    "  - Surface temperature (2t)\n",
    "  - Wind components (10u, 10v) - note potential component swapping issue\n",
    "  - Cloud cover at different levels (hcc, lcc, mcc, tcc)\n",
    "  - Solar radiation (ssr, ssrd)\n",
    "  - Precipitation (tp)\n",
    "\n",
    "**Temporal Considerations**\n",
    "- Data requires timezone conversion (UTC to IST, +5.5 hours) for accurate daily pattern analysis\n",
    "- Some variables are instantaneous readings (temperature, wind) while others are accumulated (precipitation, radiation)\n",
    "\n",
    "**Spatial Alignment**\n",
    "- Customer locations don't directly align with ERA5 grid points\n",
    "- ERA5-Land locations also have a different grid structure\n",
    "- Spatial joining methodology needed to match locations to nearest weather data points\n",
    "\n",
    "**Next Steps**\n",
    "- Convert data to parquet format for more efficient storage and retrieval\n",
    "- Implement spatial joining methodology for ERA5-Land\n",
    "- Use dataset for gap filling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irpenv_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
